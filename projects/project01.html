<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="/assets/img/favicon_io/favicon.ico">

    <title>Kenneth Chang | Projects | Cartographer </title>

    <!-- Bootstrap core CSS -->
    <link href="../assets/css/bootstrap.css" rel="stylesheet">


    <!-- Custom styles for this template -->
    <link href="../assets/css/main.css" rel="stylesheet">

    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="../assets/js/hover.zoom.js"></script>
    <script src="../assets/js/hover.zoom.conf.js"></script>

    <!--headerfooterscript-->
    <script src="/assets/js/HeaderFooterLoad.js"></script>

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <!-- Static navbar -->
    <div class="navbar navbar-inverse navbar-static-top" >
      <div class="container">
        <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">Kenneth Chang</a>
        </div>
        <div class="navbar-collapse collapse">
        <ul id="headernavbar" class="nav navbar-nav navbar-right">
        </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>
	
	
	<!-- +++++ Projects Section +++++ -->
	
	<div class="container pt">
		<div class="row mt">
			<div class="">
				<h1>Cartographer</h1>
				<hr>
				<p>Cartographer is a project that I started after becoming interested in the marketing aspects of a business, particularly in regards to Key Opinion Leaders, Community Leaders, and Influencers.
          A big portion of marketing nowdays is related to identifying and marketing through these individuals as opposed to things like ads. However, when I inquired about a quantitative method of identifying 
        people like this at my work, it became apparent that much of this is still manually done. Rather then any quantitative approach, it more boiled down into a networking excecise then an analytical one. </p>
        <p>Seeing this as a potential avenue for improvement, or even just an alternative way to do things, this project was borne. The goal of this project was to explore quantitative methods for finding Key Opinion Leaders
          ,map their influence across a topic space, and granularly explore a certain KOL's sphere of influence. These methods could then be hopefully translated into a tool that marketing and market research analysts could then use. 
        </p>
			</div>
		</div>
    <hr>
    <div class="row mt">	
			<div>
        <h2 id=sec1 alt="">Starting Out</h2>
				<p>
          The first challenge was to figure out what exactly to measure.
          Influence is a fickle and weird thing to measure.
          Many methods are very manual, such as sending out surveys to a representative sample.
          In social media contexts, raw numbers are often used as a substitute, the more followers you have, the more influence you have.
          However, that ran anti-thetical to what I wanted to build.
          I wanted granualar analysis, not just reducing people down to a metric.
          Besides, my initial research into this already revealed that something like this existed, the h-index.
          The h-index can be understood as a measure of how many citations an author received relative to how many articles they published.
          And it was here that an idea struck me.
          I could implement all the citation relations into a graph database, with each citation representing a link and each publication a node.
          Then, I could do what Google does with their webpages, and use PageRank (or another centrality algorithm) to find which articles have the highest weights.
          The higher the weight, the more central, and therefore the more important and influential the author is.
          As an added bonus, the graph database would allow me to look for things like pathing, community grouping, etc. 
        </p>

      </div>
    </div><!-- /row -->
    <hr>

    <div class="row mt">	
			<div>
        <h2 id=sec2 alt="">Pulling Data</h2>
				<p>
          The first major effort was pulling data to construct the relational database. My initial thought was to try pulling data from google scholar, since they looked like to had the most comprehensive listing 
          of publications. But here I ran into my first problem, namely that the data I was looking for wasn't always available. Google Scholar had no available api, so I had to resort to writing a selenium webdriver script to scrape the website. 
          But google was very aggressive in weeding out scraping scripts. I tried several solutions to evade this, including having the script randomly perform actions, and routing the connection through TOR, but I ultimately concluded that it wasn't worth the time and effort I was putting in. 
          I was spending more time fighting Google then actually building the database.<a> script here </a> And so I went looking for other solutions. 
        </p>
        <p>
           This turned out to be the correct decision, as more searching revelaed the presence of data sources WITH API's, namely Pubmed and Crossref. Both had api's that importantly not only listed an article's references, but also citations from other articles. 
           I was able to use those api's to write a script to pull articles. I initially structured to script to pull recursively from a single "seed" article, that is it would pull the seed's citations and references, and then use those pull their own citations and references, and so on and so forth.
           However, I quickly discovered it was a lot easier to just use Pubmed's term search function simulating a keyword search to pull an entire list first and go from there. To start out, I used the keyword "Urology", since I was already familiar with the space from work and could use that as sort of a validation that I was pulling 
           relevant articles. I quickly discovered that within one recursion, I was already starting to get non-urology related articles, and the number of articles was also starting to inrease exponentially, so I cut if off at 1 (confirm) recursion. The final article count pulled for the initial test case was 
           approximately 100k. <a>clean up script and put it here </a>
        </p>

        
      </div>
      
    </div><!-- /row -->

    <hr>

    
    <div class="row mt">	
			<div>
        <h2 id=sec3 alt="">Creating the Graph Database</h2>
				<p>
          using neo4j to create an relation database, including the insertion scripts, 
          For creating the graph relation database, I went with using neo4j. 
          Neo4j was my first introduction to graph relation databases, and rather than try and re-learn an entirely different system, I decided to go with what I was familiar with.
          Setup of the database wasquite simple, with nodes created for authors, publications, and journals.
          I also made the decision to use separate nodes for associated keywords and the publication type, rather then just implementing it as a property of the node. 
          
        </p>
        <p>
          One of those improvements was to drastically reduce the time needed to insert new nodes into the db. 
          The first improvement was to implement indexing on the database. 
          The next was to simplify the insertion process. 
          I had previously written it as an insertion script, but looking through the neo4j documentation, it seemed like at a large enough scale, it was much easier to just use the Load CSV function.
          This reduced many lines of code down to a line of Cypher, and helped greatly simplify the process. 
        </p>
        <p>
          Since I had intended this as a prototype run, I implemented certain prepcoressing/data cleaning steps but had to skip others. 
          This ultimately came down to a prioritization effort. Some were relatively easy to do, some required a lot more effort. 
          There was also differing importance. Some would be vital to this prototype, others were more nice to have. 

          The main cleaning step I did was to combine author names together where I could.
          Author names were not always presented in the same format. 
          Some used Full First name, last name, some had middle initials, some abbreviated the first name, etc. 
          I implemented the most basic clean, combining if their first and last names matched. (expand)
          This took care of most of authors, but especially for those with abbreviated first names, which would require a more detailed disambiguation effort, I skipped for now. 
          This is also apparently a major issue, with some major research effort using ML put into it (link)

          Date disambiguation was relatively straightforward to do, but I purposefully deprioritized. 
          While it was relatively simple, there was a wide variety of date formats used, and some more precise, going down to the exact day rather then month. 
          Looking at graph changes over a time series was something that I was interested in, but I thought that it was important to get the basic graph db structure implemented to test it out.
          Time series analysis could be done later. 

          The final piece of cleaning that could have been done was address/institution disambiguation. 
          I had originally inteded to use this to allow association of influence with certain hospitals, universities, etc. 
          However this turned out to be far more difficult than anticipated.
          Just like authors and dates, there was a wide variety of formats and precision that were detailed.
          In one case, I had an article with three authors, and despite all belonging to the same institution, listed it three different ways. 
          This became nightmare to disambiguate. 
          I explored some different methods of doing so, including by looking at things like cosine similarity between the different listed, and combining them if they met a threshold, but there was no way that consistently worked for all edge cases.
          Given the rickety state of this, I ultimately decided to skip it for the prototype phase. 
          I really wanted to implement this though, and this is probably the first additional feature that I would want to go back and work on. 

          </p>
        
      </div>
      
    </div><!-- /row -->
    <hr>
    <div class="row mt">	
			<div>
        <h2 id=sec4 alt="">Analytics</h2>
				<p>
          
          After the database prototype was set up, next came time to run analysis and use the database.
          The first effort consisted of running PageRank on the graph.
          The second consisted of creating pre-set cypher queries that could be used, ie find all articles by a single author related to a certain topic.
          In programming terms, this was relatively straightforward, mainly consited of creating different queries.
          
        </p>
        <p>
          To better simulate a use case, I approached from the perspective of an analyst tasked with putting together a profile of an author.
          I purposfully chose one that we already worked with at my day job, since this would allow me to validate my findings.

          Here the graph allowed for some interesting analysis that would be alot more difficult to do otherwise.
          The first was looking at the collaboration graph.
          This let me see not only just who collaborated with who, but also allowed for identifying communities around the author.
          The graph also allowed me to more easily analyze influence: who an author cited, who cited them, and on what topics.   
          You can see the final report that I put together at the Google Drive link below: <br>
          <a href="https://drive.google.com/file/d/1bb5DIm9BjjmYwgM7HVslVs5ZhpzzUwRn/view?usp=sharing">Sample Author Profile</a>
        </p>
        

        
      </div>
      
    </div><!-- /row -->
    <hr>
    <div class="row mt">	
			<div>
        <h2 id=sec5 alt="">Feedback and Future Improvements</h2>

        <P>
          After finishing up the prototype, I looked to try and get validation on my idea from marketing professionals I knew. 
          
        </P>
        <p>Overall, the feedback was positive, with them remarking that they could see the value-add in the tool.
          Particularly, they remarked that it would allow for a more analytical and quantitative method of going about doing their market research, particularly where it would usually be a networking exercise.
          They also mentioned it would also be a useful tool when planning and executing a publication strategy.
          
        </p>
				<p>
          There were however improvements and limitations that they mentioned, that I took to heart, and am planning improvements for.  
        </p>
        <p>
          The first issue was that scientific research publications are often not the main focus in Marketing.
          Much more commonly used are things like syndicated reports talking about market trends, pricing and financials info, and mergers and acquisitions news.
          In hindsight, this makes sense because a lot of marketing activity can be boiled down to looking for new opportunities.
          Cartographer's focus on a person's influence means that it does not contribute as much there. 
        </p>
        <p>
          The other limitation is on when the tool could be useful.
          Talking with professionals, there would be two useful periods of time.
          The first is early on, when a team needs to get the lay of the land they are working in.
          The second is later on when the project is scaled up, and when there are enough marketing resources, like a full department, to spare to do precision targeting of publications and physicians.
          This meant that there was a "middle" point, where Cartographer wouldn't be as useful. 
        </p>
        <p>
          However, this gave me new ideas on improvements that I could make that could allow it to be helpful in this regard. 
        </p>
        <p>
          To address the first limitation, I would expand the articles used beyond just scientific publications.
          This would be the most straightforward improvement, though the most technically challenging, particularly from a data collection standpoint.
          A dedicated web crawler would have to be created, that constantly looked for new articles, reports, etc in a desired topic space, which would then feed into the database.
          Citation links would be replaced instead by topic references.
          Analysis would focus less on authors, and more on topics, ie which terms are showing up more. 
        </p>
        <p>
          The other improvements would be from an analytical perspective, which would help address the two limitations.
          The date cleaning that I skipped would have to be implemented, to allow for the graph to be observed from a time series perspective.
          This would allow an analyst to watch how the graph changes over time, including what topics are seeing more interest.
          Combining this with slicing the graph by keywords, and term mentions in the abstracts, would allow for modeling of interest over time.
          Better organization and cleaning of keywords and topics, including better preprocessing like lemmatization would also be implemented.
          The graph structure would also allow better analysis of how ideas spread: looking for originators of ideas, and looking at how they spread between authors. 
        </p>
        <p>
          These changes would ultimately allow an analyst to easily pick up on new trends, or analyze historical ones to gain insight on how the market might change, or neglected spaces.
          This would be useful no matter where in the life cycle of the project. 
        </p>
        <p>
          Ultimately, I hope to implement the improvements to address the feedback and improve Cartographer.
        </p>
         
      </div>
      
    </div><!-- /row -->
		
	</div><!-- /container -->
	
	
		<!-- +++++ Footer Section +++++ -->
	
    <div id="footer">
      <div class="container">
        <div class="row">
          <div id="addressLine" class="col-lg-4">
            <h4>Living at</h4>
          </div><!-- /col-lg-4 -->
          
          <div  class="col-lg-4">
            <h4>My Links</h4>
            <ul id="footerLinks">
            </ul>
          </div><!-- /col-lg-4 -->
        </div>
      </div>
    </div>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="assets/js/bootstrap.min.js"></script>
  </body>
</html>
