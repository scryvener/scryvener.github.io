<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../docs-assets/ico/favicon.png">

    <title>Kenneth Chang | Projects | Cartographer </title>

    <!-- Bootstrap core CSS -->
    <link href="../assets/css/bootstrap.css" rel="stylesheet">


    <!-- Custom styles for this template -->
    <link href="../assets/css/main.css" rel="stylesheet">

    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="../assets/js/hover.zoom.js"></script>
    <script src="../assets/js/hover.zoom.conf.js"></script>

    <!--headerfooterscript-->
    <script src="/assets/js/HeaderFooterLoad.js"></script>

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <!-- Static navbar -->
    <div class="navbar navbar-inverse navbar-static-top" >
      <div class="container">
        <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">Kenneth Chang</a>
        </div>
        <div class="navbar-collapse collapse">
        <ul id="headernavbar" class="nav navbar-nav navbar-right">
        </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>
	
	
	<!-- +++++ Projects Section +++++ -->
	
	<div class="container pt">
		<div class="row mt">
			<div class="">
				<h1>Cartographer</h1>
				<hr>
				<p>Cartographer is a project that I started after becoming interested in the marketing aspects of a business, particularly in regards to Key Opinion Leaders, Community Leaders, and Influencers.
          A big portion of marketing nowdays is related to identifying and marketing through these individuals as opposed to things like ads. However, when I inquired about a quantitative method of identifying 
        people like this at my work, it became apparent that much of this is still manually done. Rather then any quantitative approach, it more boiled down into a networking excecise then an analytical one. </p>
        <p>Seeing this as a potential avenue for improvement, or even just an alternative way to do things, this project was borne. The goal of this project was to explore quantitative methods for finding Key Opinion Leaders
          ,map their influence across a topic space, and granularly explore a certain KOL's sphere of influence. These methods could then be hopefully translated into a tool that marketing and market research analysts could then use. 
        </p>
			</div>
		</div>
    <hr>
    <div class="row mt">	
			<div>
        <h2 id=sec1 alt="">Starting Out</h2>
				<p>
          the planning part
          The first challenge was to figure out what exactly to measure.
          Influence is a fickle and weird thing to measure.
          Many methods are very manual, such as sending out surveys to a representative sample.
          In social media contexts, raw numbers are often used as a substitute, the more followers you have, the more influence you have.
          However, that ran anti-thetical to what I wanted to build.
          I wanted granualar analysis, not just reducing people down to a metric.
          Besides, my initial research into this already revealed that something like this existed, the h-index.
          The h-index can be understood as a measure of how many citations an author received relative to how many articles they published.
          And it was here that an idea struck me.
          I could implement all the citation relations into a graph database, with each citation representing a link and each publication a node.
          Then, I could do what Google does with their webpages, and use PageRank (or another centrality algorithm) to find which articles have the highest weights.
          The higher the weight, the more central, and therefore the more important and influential the author is.
          As an added bonus, the graph database would allow me to look for things like pathing, community grouping, etc. 
        </p>

      </div>
    </div><!-- /row -->
    <hr>

    <div class="row mt">	
			<div>
        <h2 id=sec2 alt="">Pulling Data</h2>
				<p>
          The first major effort was pulling data to construct the relational database. My initial thought was to try pulling data from google scholar, since they looked like to had the most comprehensive listing 
          of publications. But here I ran into my first problem, namely that the data I was looking for wasn't always available. Google Scholar had no available api, so I had to resort to writing a selenium webdriver script to scrape the website. 
          But google was very aggressive in weeding out scraping scripts. I tried several solutions to evade this, including having the script randomly perform actions, and routing the connection through TOR, but I ultimately concluded that it wasn't worth the time and effort I was putting in. 
          I was spending more time fighting Google then actually building the database.<a> script here </a> And so I went looking for other solutions. 
        </p>
        <p>
           This turned out to be the correct decision, as more searching revelaed the presence of data sources WITH API's, namely Pubmed and Crossref. Both had api's that importantly not only listed an article's references, but also citations from other articles. 
           I was able to use those api's to write a script to pull articles. I initially structured to script to pull recursively from a single "seed" article, that is it would pull the seed's citations and references, and then use those pull their own citations and references, and so on and so forth.
           However, I quickly discovered it was a lot easier to just use Pubmed's term search function simulating a keyword search to pull an entire list first and go from there. To start out, I used the keyword "Urology", since I was already familiar with the space from work and could use that as sort of a validation that I was pulling 
           relevant articles. I quickly discovered that within one recursion, I was already starting to get non-urology related articles, and the number of articles was also starting to inrease exponentially, so I cut if off at 1 (confirm) recursion. The final article count pulled for the initial test case was 
           approximately 100k. <a>clean up script and put it here </a>
        </p>

        
      </div>
      
    </div><!-- /row -->

    <hr>

    
    <div class="row mt">	
			<div>
        <h2 id=sec3 alt="">Creating the Graph Database</h2>
				<p>
          using neo4j to create an relation database, including the insertion scripts, 
          For creating the graph relation database, I went with using neo4j. 
          Neo4j was my first introduction to graph relation databases, and rather than try and re-learn an entirely different system, I decided to go with what I was familiar with.
          Setup of the database wasquite simple, with nodes created for authors, publications, and journals.
          I also made the decision to use separate nodes for associated keywords and the publication type, rather then just implementing it as a property of the node. 
          
        </p>
        <p>figureing out faster ways of doing things (indexing, load csv, etc. )
          One of those improvements was to drastically reduce the time needed to insert new nodes into the db. 
          The first improvement was to implement indexing on the database. 
          The next was to simplify the insertion process. 
          I had previously written it as an insertion script, but looking through the neo4j documentation, it seemed like at a large enough scale, it was much easier to just use the Load CSV function.
          This reduced many lines of code down to a line of Cypher, and helped greatly simplify the process. 
        </p>
        <p>
          Since I had intended this as a prototype run, I implemented certain prepcoressing/data cleaning steps but had to skip others. 
          This ultimately came down to a prioritization effort. Some were relatively easy to do, some required a lot more effort. 
          There was also differing importance. Some would be vital to this prototype, others were more nice to have. 

          The main cleaning step I did was to combine author names together where I could.
          Author names were not always presented in the same format. 
          Some used Full First name, last name, some had middle initials, some abbreviated the first name, etc. 
          I implemented the most basic clean, combining if their first and last names matched. (expand)
          This took care of most of authors, but especially for those with abbreviated first names, which would require a more detailed disambiguation effort, I skipped for now. 
          This is also apparently a major issue, with some major research effort using ML put into it (link)

          Date disambiguation was relatively straightforward to do, but I purposefully deprioritized. 
          While it was relatively simple, there was a wide variety of date formats used, and some more precise, going down to the exact day rather then month. 
          Looking at graph changes over a time series was something that I was interested in, but I thought that it was important to get the basic graph db structure implemented to test it out.
          Time series analysis could be done later. 

          The final piece of cleaning that could have been done was address/institution disambiguation. 
          I had originally inteded to use this to allow association of influence with certain hospitals, universities, etc. 
          However this turned out to be far more difficult than anticipated.
          Just like authors and dates, there was a wide variety of formats and precision that were detailed.
          In one case, I had an article with three authors, and despite all belonging to the same institution, listed it three different ways. 
          This became nightmare to disambiguate. 
          I explored some different methods of doing so, including by looking at things like cosine similarity between the different listed, and combining them if they met a threshold, but there was no way that consistently worked for all edge cases.
          Given the rickety state of this, I ultimately decided to skip it for the prototype phase. 
          I really wanted to implement this though, and this is probably the first additional feature that I would want to go back and work on. 

          </p>
        
      </div>
      
    </div><!-- /row -->
    <hr>
    <div class="row mt">	
			<div>
        <h2 id=sec4 alt="">Analytics</h2>
				<p>
          using the db, ie running Pagerank, etc. link our sample analytical product 
          Probably the most interesting part of my work was related to supporting Pre-Clinical and Clinical Trials, and not just because I got to travel.
          Any medical device development is always accompanied by testing both in-vivo and in-vitro.
          At Prodeon Medical, I had the opportunity to be directly involved in cadaver, animal, and First-in-Human clinical trials.
        </p>
        <p>
          Organ cadavers were my first exposure to testing outside the bench, and was an invaluable tool in evaluating early designs.
          Even the best bench models had limitations, and so these tests allowed us to rapidly screen out designs and test the feasibility of our proposed procedure.
          Our extensive cadaver studies helped immensely in allowing us to succesfully start a longer term animal study within a year or so of the project start.
        </p>

        <P>
          I was extensively involved our animal trials, using the knowledge gained to evaluate potential designs, and to test the long-term efficacy of the implant. Here I manufactured and prepared devices to be used, 
          worked with the vetinary surgeon to implant the devices, and analyzed the collected data such as x-rays. I was also responsible for defining and executing our post-retrieval testing for the implants, which included 
          mechanical testing, and SEM imaging of the surface.
          
        </P>
        <p>
          When the time came for clinical trials, I was on the ground in Europe, Asia, and Australia training the first physicians for the First in Human <a href="https://clinicaltrials.gov/ct2/show/NCT03758222">EXPANDER-1 Trial</a>.
          I also provided remote technical support and helped execute device deficiency investigations.
        </p>

        
      </div>
      
    </div><!-- /row -->
    <hr>
    <div class="row mt">	
			<div>
        <h2 id=sec5 alt="">future efforts and things to improve on </h2>

        <P>
          talking with actual marketing friends-actaul way to expand this to non citations? ie maybe mentions in publichsed articles? from the db perspective this is easy, from the dat agathering perspective Im not sure how I would go about doing this, besides a mass scrape. 
          
          talking with marketing experienced- very valuable for initial starting out and for larger marketing teams, less so for middle ground(know the space already). Thinking about adding more features, such as more info on the articles, ie pulling topics from the abstracts,
          better implmementing keywords(maybe lemmatized) to allow better segmentation. 
          
        </P>
        <p>better preprocessing mentioned above
          </p>
				<p>
          pull way more articles, not just from a single seed. will need to be scaled up, since was already taking ahot minute 
          
        </p>
         
      </div>
      
    </div><!-- /row -->
		
	</div><!-- /container -->
	
	
		<!-- +++++ Footer Section +++++ -->
	
    <div id="footer">
      <div class="container">
        <div class="row">
          <div id="addressLine" class="col-lg-4">
            <h4>Living at</h4>
          </div><!-- /col-lg-4 -->
          
          <div  class="col-lg-4">
            <h4>My Links</h4>
            <ul id="footerLinks">
            </ul>
          </div><!-- /col-lg-4 -->
        </div>
      </div>
    </div>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="assets/js/bootstrap.min.js"></script>
  </body>
</html>
